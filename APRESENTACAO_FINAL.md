# APRESENTAÃ‡ÃƒO FINAL - SISTEMA DE RECOMENDAÃ‡ÃƒO DE ANIMES
## Roteiro Completo de ApresentaÃ§Ã£o

**Projeto:** Sistema de RecomendaÃ§Ã£o de Animes  
**ResidÃªncia em TI - TJGO**  
**DuraÃ§Ã£o Total:** 10 minutos  
**Metodologia:** CRISP-DM

---

## INTRODUÃ‡ÃƒO - [Tempo estimado: 1 minuto]

**ğŸ‘¤ Apresentador: Ariel**

### Slide 1: Capa e ApresentaÃ§Ã£o da Equipe

**ConteÃºdo do Slide:**
- **TÃ­tulo:** Sistema de RecomendaÃ§Ã£o de Animes
- **SubtÃ­tulo:** AplicaÃ§Ã£o de Machine Learning com Filtragem Colaborativa
- **Equipe:**
  - Ariel Angelo Guiliane Mendes de Almeida
  - JoÃ£o Pedro JosÃ© Santos da Silva Guedes
- **ResidÃªncia em TI - TJGO**
- **Dezembro 2024**

**Recursos Visuais:**
- ğŸ¨ Background com tema de animes (sutil e profissional)
- ğŸ›ï¸ Logo do TJGO
- ğŸ‘¥ Ãcones representando a equipe

**Fala do Apresentador:**
Bom dia a todos! Meu nome Ã© [Nome] e junto com meu colega [Nome do colega], desenvolvemos um Sistema de RecomendaÃ§Ã£o de Animes como projeto final da ResidÃªncia em TI do TJGO. Nosso objetivo foi aplicar tÃ©cnicas de Machine Learning para resolver um problema real: como ajudar usuÃ¡rios a descobrir novos conteÃºdos em um catÃ¡logo com mais de 24 mil animes. Utilizamos a metodologia CRISP-DM e alcanÃ§amos resultados excepcionais que vou apresentar nos prÃ³ximos minutos.

---

### Slide 2: Agenda e Contexto

**ConteÃºdo do Slide:**
- **Agenda:**
  1. ğŸ¯ Problema de NegÃ³cio (Ariel)
  2. ğŸ“Š Dados e AnÃ¡lise (JoÃ£o Pedro)
  3. ğŸ”§ PreparaÃ§Ã£o e Pipeline (Ariel)
  4. ğŸ¤– Duas Abordagens de RecomendaÃ§Ã£o (JoÃ£o Pedro)
  5. ğŸ“ˆ ComparaÃ§Ã£o e Resultados (JoÃ£o Pedro)
  6. ğŸš€ ConclusÃµes e RecomendaÃ§Ãµes (Ariel)

- **Metodologia:** CRISP-DM
- **Dataset:** MyAnimeList 2023 (Kaggle)
- **Destaque:** ComparaÃ§Ã£o entre User-Based CF e Item-Based CF

**Recursos Visuais:**
- ğŸ“‹ Diagrama circular do CRISP-DM
- ğŸ”— Logo do Kaggle e MyAnimeList
- âš–ï¸ Ãcone de comparaÃ§Ã£o/balanÃ§a

**Fala do Apresentador:**
Nossa apresentaÃ§Ã£o seguirÃ¡ as seis fases da metodologia CRISP-DM, que Ã© o padrÃ£o da indÃºstria para projetos de Data Mining. Um diferencial do nosso projeto Ã© que implementamos e comparamos DUAS abordagens de filtragem colaborativa: User-Based e Item-Based. Vamos comeÃ§ar entendendo o problema de negÃ³cio, passar pela anÃ¡lise e preparaÃ§Ã£o dos dados, apresentar ambas as abordagens, comparar os resultados obtidos e finalizar com recomendaÃ§Ãµes prÃ¡ticas. Utilizamos o dataset pÃºblico do MyAnimeList disponÃ­vel no Kaggle, que contÃ©m mais de 24 milhÃµes de avaliaÃ§Ãµes de usuÃ¡rios.

---

## 1. ENTENDIMENTO DO NEGÃ“CIO - [Tempo estimado: 1 minuto]

**ğŸ‘¤ Apresentador: Ariel**

### Slide 3: O Problema

**ConteÃºdo do Slide:**
- **Desafio:** Como recomendar animes relevantes em um catÃ¡logo de 24.905 tÃ­tulos?

- **Problemas Identificados:**
  - ğŸ“Š CatÃ¡logo muito extenso
  - ğŸ” Dificuldade de descoberta de novos conteÃºdos
  - ğŸ“‰ Baixo engajamento dos usuÃ¡rios
  - ğŸ’° Impacto na retenÃ§Ã£o e satisfaÃ§Ã£o

- **SoluÃ§Ã£o:** Sistema de RecomendaÃ§Ã£o Inteligente

**Recursos Visuais:**
- ğŸ“Š GrÃ¡fico mostrando crescimento do catÃ¡logo de animes ao longo dos anos
- ğŸ¯ Ãcone de usuÃ¡rio perdido em meio a muitas opÃ§Ãµes

**Fala do Apresentador:**
O problema que nos propusemos a resolver Ã© muito comum em plataformas de streaming: como ajudar usuÃ¡rios a encontrar conteÃºdo relevante quando hÃ¡ milhares de opÃ§Ãµes disponÃ­veis? No caso dos animes, temos mais de 24 mil tÃ­tulos no dataset. UsuÃ¡rios gastam muito tempo procurando o que assistir, muitos acabam desistindo, e isso impacta diretamente o engajamento e a satisfaÃ§Ã£o. Nossa soluÃ§Ã£o foi desenvolver um sistema de recomendaÃ§Ã£o inteligente que aprende com os padrÃµes de comportamento dos usuÃ¡rios.

---

### Slide 4: Objetivos e MÃ©tricas de Sucesso

**ConteÃºdo do Slide:**
- **Objetivos:**
  1. âœ… Personalizar recomendaÃ§Ãµes por usuÃ¡rio
  2. âœ… Comparar duas abordagens de filtragem colaborativa
  3. âœ… Reduzir tempo de busca e aumentar satisfaÃ§Ã£o
  4. âœ… Identificar trade-offs entre acurÃ¡cia e performance

- **Metas de Sucesso:**
  - MAE (Mean Absolute Error) < 1.5
  - RMSE (Root Mean Squared Error) < 2.0
  - Cobertura > 1.000 usuÃ¡rios
  - Tempo de prediÃ§Ã£o < 5ms por rating

**Recursos Visuais:**
- ğŸ¯ Ãcones para cada objetivo
- ğŸ“Š Tabela de mÃ©tricas com metas definidas
- âš–ï¸ Ãcone de balanÃ§a para comparaÃ§Ã£o

**Fala do Apresentador:**
Definimos quatro objetivos principais: personalizar as recomendaÃ§Ãµes baseadas no histÃ³rico de cada usuÃ¡rio, comparar duas abordagens diferentes de filtragem colaborativa para identificar qual funciona melhor, reduzir o tempo de busca e aumentar a satisfaÃ§Ã£o, e entender os trade-offs entre acurÃ¡cia e performance. Para medir o sucesso, estabelecemos metas quantitativas: erro mÃ©dio absoluto menor que 1.5, erro quadrÃ¡tico mÃ©dio menor que 2.0, capacidade de atender pelo menos mil usuÃ¡rios, e tempo de prediÃ§Ã£o menor que 5 milissegundos. Spoiler: AMBAS as abordagens superaram todas essas metas!

---

## 2. ENTENDIMENTO DOS DADOS - [Tempo estimado: 1.5 minutos]

**ğŸ‘¤ Apresentador: JoÃ£o Pedro**

### Slide 5: Dataset e Estrutura

**ConteÃºdo do Slide:**
- **Fonte:** MyAnimeList Dataset 2023 (Kaggle)

- **Dados de Animes:**
  - 24.905 animes
  - 24 atributos (nome, gÃªnero, score, estÃºdio, etc.)

- **Dados de UsuÃ¡rios:**
  - MilhÃµes de avaliaÃ§Ãµes
  - Escala: 1 a 10
  - 5 atributos (user_id, anime_id, rating, etc.)

**Recursos Visuais:**
- ğŸ“Š Tabela mostrando exemplo de dados de animes
- ğŸ“Š Tabela mostrando exemplo de avaliaÃ§Ãµes de usuÃ¡rios
- ğŸ“ˆ NÃºmeros destacados: 24.905 animes, milhÃµes de ratings

**Fala do Apresentador:**
Trabalhamos com o MyAnimeList Dataset 2023, disponÃ­vel no Kaggle. Ele contÃ©m dois conjuntos principais: dados dos animes, com quase 25 mil tÃ­tulos e 24 atributos como nome, gÃªnero, score mÃ©dio e estÃºdio; e dados de usuÃ¡rios, com milhÃµes de avaliaÃ§Ãµes em uma escala de 1 a 10. Esse volume de dados nos permitiu treinar um modelo robusto e fazer recomendaÃ§Ãµes precisas.

---

### Slide 6: AnÃ¡lise ExploratÃ³ria - Principais Insights

**ConteÃºdo do Slide:**
- **Insights Descobertos:**

1. **Esparsidade:** Densidade < 1%
   - Maioria dos usuÃ¡rios avalia poucos animes

2. **DistribuiÃ§Ã£o de Ratings:** ViÃ©s positivo
   - Ratings mais comuns: 7, 8, 9, 10

3. **Popularidade:** DistribuiÃ§Ã£o long tail
   - Poucos animes concentram maioria das avaliaÃ§Ãµes

4. **Desafio:** Cold Start Problem

**Recursos Visuais:**
- ğŸ“Š Histograma da distribuiÃ§Ã£o de ratings (mostrando viÃ©s para valores altos)
- ğŸ“ˆ GrÃ¡fico de cauda longa da popularidade dos animes
- ğŸ”¥ Heatmap mostrando esparsidade da matriz usuÃ¡rio-item

**Fala do Apresentador:**
Durante a anÃ¡lise exploratÃ³ria, identificamos quatro insights importantes. Primeiro, a matriz de dados Ã© extremamente esparsa - menos de 1% das cÃ©lulas tÃªm valores, pois a maioria dos usuÃ¡rios avalia poucos animes. Segundo, hÃ¡ um viÃ©s positivo nas avaliaÃ§Ãµes: as pessoas tendem a avaliar melhor os animes que escolhem assistir, com notas 7, 8, 9 e 10 sendo as mais comuns. Terceiro, observamos uma distribuiÃ§Ã£o de cauda longa na popularidade: poucos animes como One Piece, Naruto e Attack on Titan concentram a maioria das avaliaÃ§Ãµes. E quarto, identificamos o desafio do Cold Start - dificuldade em recomendar para usuÃ¡rios ou animes novos sem histÃ³rico. Esses insights guiaram nossas decisÃµes na preparaÃ§Ã£o dos dados.

---

## 3. PREPARAÃ‡ÃƒO DOS DADOS - [Tempo estimado: 1.5 minutos]

**ğŸ‘¤ Apresentador: Ariel**

### Slide 7: Pipeline de PrÃ©-processamento

**ConteÃºdo do Slide:**
- **Abordagem:** Pipelines Scikit-learn

- **5 Transformadores Customizados:**
  1. ğŸ”¢ **UserAnimeIndexMapper** - Mapeia IDs para Ã­ndices
  2. ğŸ“Š **SparseMatrixCreator** - Cria matriz esparsa CSR
  3. ğŸ‘¥ **TopUserSelector** - Seleciona 5.000 usuÃ¡rios mais ativos
  4. âš–ï¸ **RatingCentralizer** - Centraliza ratings por usuÃ¡rio
  5. ğŸ”— **SimilarityMatrixCalculator** - Calcula similaridade de cosseno

- **BenefÃ­cios:** Reprodutibilidade, Modularidade, Manutenibilidade

**Recursos Visuais:**
- ğŸ”„ Diagrama de fluxo mostrando o pipeline completo
- ğŸ“¦ Ãcones para cada transformador
- âœ… Checkmarks destacando os benefÃ­cios

**Fala do Apresentador:**
Para preparar os dados, desenvolvemos um pipeline completo usando Scikit-learn, que Ã© uma biblioteca padrÃ£o de Machine Learning em Python. Criamos cinco transformadores customizados, cada um com uma responsabilidade especÃ­fica. O primeiro mapeia os IDs de usuÃ¡rios e animes para Ã­ndices numÃ©ricos. O segundo cria uma matriz esparsa no formato CSR, que Ã© muito eficiente em termos de memÃ³ria. O terceiro seleciona os 5 mil usuÃ¡rios mais ativos - essa amostragem foi necessÃ¡ria para viabilizar o processamento em tempo razoÃ¡vel. O quarto centraliza os ratings pela mÃ©dia de cada usuÃ¡rio, removendo vieses individuais. E o quinto calcula a matriz de similaridade usando cosseno. Essa abordagem de pipeline nos dÃ¡ reprodutibilidade, modularidade e facilita a manutenÃ§Ã£o do cÃ³digo.

---

### Slide 8: DecisÃµes TÃ©cnicas Importantes

**ConteÃºdo do Slide:**
- **Por que 5.000 usuÃ¡rios?**
  - âœ… Balanceamento qualidade vs. performance
  - âœ… Tempo de processamento: ~2 minutos
  - âœ… UsuÃ¡rios mais ativos = mais dados confiÃ¡veis

- **Por que centralizaÃ§Ã£o de ratings?**
  - âœ… Remove viÃ©s individual (alguns avaliam sempre alto/baixo)
  - âœ… Foca em preferÃªncias relativas

- **Por que matriz esparsa?**
  - âœ… Economia de memÃ³ria (densidade < 1%)
  - âœ… OperaÃ§Ãµes otimizadas

**Recursos Visuais:**
- âš–ï¸ GrÃ¡fico de balanÃ§a mostrando trade-off qualidade vs. performance
- ğŸ“Š ComparaÃ§Ã£o de uso de memÃ³ria: matriz densa vs. esparsa
- ğŸ¯ Exemplo visual de centralizaÃ§Ã£o de ratings

**Fala do Apresentador:**
TrÃªs decisÃµes tÃ©cnicas foram fundamentais. Primeiro, por que apenas 5 mil usuÃ¡rios? Fizemos um balanceamento entre qualidade e performance. Com 5 mil usuÃ¡rios mais ativos, conseguimos processar tudo em cerca de 2 minutos e ainda ter dados suficientes para recomendaÃ§Ãµes precisas. Segundo, por que centralizar os ratings? Porque cada pessoa tem um viÃ©s individual - alguns avaliam sempre alto, outros sempre baixo. Ao centralizar, focamos nas preferÃªncias relativas de cada usuÃ¡rio, nÃ£o nos valores absolutos. E terceiro, por que usar matriz esparsa? Porque com densidade menor que 1%, uma matriz densa desperdiÃ§aria 99% da memÃ³ria armazenando zeros. A matriz esparsa nos dÃ¡ economia de memÃ³ria e operaÃ§Ãµes otimizadas.

---

## 4. MODELAGEM - [Tempo estimado: 2 minutos]

**ğŸ‘¤ Apresentador: JoÃ£o Pedro**

### Slide 9: Algoritmo Escolhido

**ConteÃºdo do Slide:**
- **Algoritmo:** Filtragem Colaborativa UsuÃ¡rio-UsuÃ¡rio

- **Por que essa escolha?**
  1. âœ… **Interpretabilidade** - FÃ¡cil explicar recomendaÃ§Ãµes
  2. âœ… **EficÃ¡cia comprovada** - Amplamente usado na indÃºstria
  3. âœ… **AdequaÃ§Ã£o** - Dataset rico em avaliaÃ§Ãµes de usuÃ¡rios
  4. âœ… **ImplementaÃ§Ã£o viÃ¡vel** - NÃ£o requer infraestrutura complexa

- **PrincÃ­pio:** "UsuÃ¡rios similares gostam de animes similares"

**Recursos Visuais:**
- ğŸ‘¥ Diagrama mostrando usuÃ¡rios similares conectados
- ğŸ¬ Exemplo visual: UsuÃ¡rio A e B gostam dos mesmos animes â†’ recomendar para A o que B gostou
- â­ Ãcone de estrelas representando ratings

**Fala do Apresentador:**
Escolhemos o algoritmo de Filtragem Colaborativa baseada em usuÃ¡rios. Esse algoritmo funciona com um princÃ­pio simples: se dois usuÃ¡rios avaliaram vÃ¡rios animes de forma similar, provavelmente eles tÃªm gostos parecidos. EntÃ£o, podemos recomendar para um usuÃ¡rio os animes que usuÃ¡rios similares gostaram. Escolhemos essa abordagem por quatro razÃµes: primeiro, Ã© muito interpretÃ¡vel - conseguimos explicar facilmente por que algo foi recomendado. Segundo, Ã© uma tÃ©cnica comprovada, usada por empresas como Netflix e Amazon. Terceiro, nosso dataset Ã© perfeito para isso, com milhÃµes de avaliaÃ§Ãµes. E quarto, a implementaÃ§Ã£o Ã© viÃ¡vel sem precisar de infraestrutura complexa ou GPUs.

---

### Slide 10: Como o Algoritmo Funciona

**ConteÃºdo do Slide:**
- **4 Passos do Algoritmo:**

**1. CÃ¡lculo de Similaridade**
- Similaridade de Cosseno entre todos os usuÃ¡rios
- Valores: -1 (opostos) a +1 (idÃªnticos)

**2. IdentificaÃ§Ã£o de Vizinhos**
- Seleciona top-50 usuÃ¡rios mais similares
- Filtra quem avaliou o anime alvo

**3. PrediÃ§Ã£o de Rating**
- FÃ³rmula ponderada pela similaridade
- Ajusta pela mÃ©dia do usuÃ¡rio

**4. GeraÃ§Ã£o de RecomendaÃ§Ãµes**
- Prediz ratings para animes nÃ£o assistidos
- Ordena e retorna top-10

**Recursos Visuais:**
- ğŸ”¢ Diagrama de fluxo mostrando os 4 passos
- ğŸ“ FÃ³rmula matemÃ¡tica da prediÃ§Ã£o (simplificada)
- ğŸ¯ Exemplo numÃ©rico com usuÃ¡rio real

**Fala do Apresentador:**
O algoritmo funciona em quatro passos. Primeiro, calculamos a similaridade de cosseno entre todos os pares de usuÃ¡rios - isso nos dÃ¡ uma matriz que indica o quÃ£o parecidos sÃ£o os gostos de cada dupla. Segundo, para fazer uma recomendaÃ§Ã£o, identificamos os 50 usuÃ¡rios mais similares ao usuÃ¡rio alvo que tambÃ©m avaliaram o anime em questÃ£o. Terceiro, fazemos a prediÃ§Ã£o do rating usando uma mÃ©dia ponderada: quanto mais similar o vizinho, mais peso sua avaliaÃ§Ã£o tem. Ajustamos tambÃ©m pela mÃ©dia histÃ³rica do usuÃ¡rio. E quarto, aplicamos esse processo para todos os animes que o usuÃ¡rio ainda nÃ£o assistiu, ordenamos por rating predito, e retornamos os top 10. Simples, mas muito eficaz!

---

### Slide 11: Exemplo Real de RecomendaÃ§Ã£o

**ConteÃºdo do Slide:**
- **UsuÃ¡rio de Teste:** ID 48
  - Total de avaliaÃ§Ãµes: 556
  - Rating mÃ©dio: 6.28
  - Favoritos: Dennou Coil (10), Fullmetal Alchemist (9)

- **Top 5 RecomendaÃ§Ãµes:**

| # | Anime | Rating Predito | GÃªnero |
|---|-------|----------------|--------|
| 1 | Ketsuinu | 10.00 | Comedy |
| 2 | Ashita no Eleven-tachi | 10.00 | Sports |
| 3 | Jakusansei Million Arthur | 9.72 | Fantasy |
| 4 | Arifureta Itsuka | 9.69 | Fantasy |
| 5 | Tsushima Maru | 9.60 | Drama |

**Recursos Visuais:**
- ğŸ‘¤ Card do perfil do usuÃ¡rio com seus animes favoritos
- ğŸ“Š Tabela destacada com as recomendaÃ§Ãµes
- ğŸ¨ Imagens pequenas dos animes recomendados (se possÃ­vel)

**Fala do Apresentador:**
Vamos ver um exemplo real da primeira abordagem, User-Based CF. Pegamos o usuÃ¡rio de ID 48, que tem 556 avaliaÃ§Ãµes e uma mÃ©dia de 6.28. Entre seus favoritos estÃ£o Dennou Coil com nota 10 e Fullmetal Alchemist com nota 9. Nosso sistema gerou essas cinco recomendaÃ§Ãµes, todas com ratings preditos muito altos, entre 9.6 e 10. Reparem na diversidade: temos comÃ©dia, esportes, fantasia e drama. Isso mostra que o sistema nÃ£o fica preso a um Ãºnico gÃªnero, mas captura a variedade de gostos do usuÃ¡rio. Essas prediÃ§Ãµes foram feitas analisando os padrÃµes dos 50 usuÃ¡rios mais similares a ele.

---

### Slide 12: Segunda Abordagem - Item-Based CF

**ConteÃºdo do Slide:**
- **Algoritmo:** Filtragem Colaborativa Item-Item (Anime-Anime)

- **DiferenÃ§a Fundamental:**
  - **User-Based:** Similaridade entre USUÃRIOS
  - **Item-Based:** Similaridade entre ANIMES

- **PrincÃ­pio:** "Animes similares agradam os mesmos usuÃ¡rios"

- **Vantagens:**
  - âš¡ **5x mais rÃ¡pido** nas prediÃ§Ãµes (0.43ms vs 2.16ms)
  - ğŸš€ **87% mais rÃ¡pido** no treinamento (7.65s vs 120s)
  - ğŸ“ˆ **Melhor escalabilidade** (matriz item-item mais estÃ¡vel)
  - ğŸ¯ **RecomendaÃ§Ãµes consistentes** com preferÃªncias estabelecidas

**Recursos Visuais:**
- ğŸ¬ Diagrama mostrando animes similares conectados
- âš–ï¸ ComparaÃ§Ã£o visual: User-Based vs Item-Based
- âš¡ GrÃ¡fico de barras comparando tempos de execuÃ§Ã£o

**Fala do Apresentador:**
Para enriquecer nossa anÃ¡lise, implementamos uma SEGUNDA abordagem: Item-Based Collaborative Filtering. A diferenÃ§a fundamental Ã© que ao invÃ©s de calcular similaridade entre usuÃ¡rios, calculamos similaridade entre animes. O princÃ­pio Ã©: se dois animes foram avaliados de forma similar por muitos usuÃ¡rios, eles sÃ£o similares. EntÃ£o, se vocÃª gostou do anime X, recomendamos animes similares a X. Essa abordagem trouxe vantagens significativas: Ã© 5 vezes mais rÃ¡pida nas prediÃ§Ãµes, 87% mais rÃ¡pida no treinamento, escala melhor porque a matriz de animes Ã© mais estÃ¡vel que a de usuÃ¡rios, e gera recomendaÃ§Ãµes mais consistentes com as preferÃªncias jÃ¡ estabelecidas do usuÃ¡rio.

---

### Slide 13: Exemplo Real - Item-Based CF

**ConteÃºdo do Slide:**
- **Mesmo UsuÃ¡rio de Teste:** ID 48 (para comparaÃ§Ã£o direta)

- **Top 5 RecomendaÃ§Ãµes (Item-Based):**

| # | Anime | Rating Predito | Score MAL | GÃªnero |
|---|-------|----------------|-----------|--------|
| 1 | Kara no Kyoukai 1 | 7.88 | 7.82 | Action, Mystery |
| 2 | Fate/stay night | 7.85 | 7.32 | Action, Fantasy |
| 3 | Ghost in the Shell | 7.80 | 8.04 | Sci-Fi, Mecha |
| 4 | Ergo Proxy | 7.78 | 7.93 | Mystery, Sci-Fi |
| 5 | Serial Experiments Lain | 7.73 | 8.01 | Sci-Fi, Mystery |

- **ObservaÃ§Ãµes:**
  - âœ… Ratings preditos mais conservadores (7.7-7.9 vs 9.6-10.0)
  - âœ… Animes com scores MAL mais altos (7.3-8.0)
  - âœ… Maior consistÃªncia de gÃªneros (Action, Sci-Fi, Mystery)

**Recursos Visuais:**
- ğŸ“Š Tabela destacada com as recomendaÃ§Ãµes Item-Based
- ğŸ¨ Imagens pequenas dos animes recomendados
- âš–ï¸ ComparaÃ§Ã£o lado a lado com User-Based

**Fala do Apresentador:**
Agora vamos ver as recomendaÃ§Ãµes da abordagem Item-Based para o MESMO usuÃ¡rio. Reparem nas diferenÃ§as: os ratings preditos sÃ£o mais conservadores, entre 7.7 e 7.9, ao invÃ©s de 9 a 10. Os animes recomendados tÃªm scores MAL mais altos, indicando que sÃ£o tÃ­tulos mais estabelecidos e bem avaliados pela comunidade. E hÃ¡ maior consistÃªncia de gÃªneros - predominam Action, Sci-Fi e Mystery, que sÃ£o gÃªneros que o usuÃ¡rio jÃ¡ demonstrou gostar. Enquanto User-Based trouxe mais diversidade e descoberta, Item-Based trouxe mais seguranÃ§a e alinhamento com preferÃªncias conhecidas. Ambas as abordagens sÃ£o vÃ¡lidas, mas para propÃ³sitos diferentes!

---

## 5. AVALIAÃ‡ÃƒO E COMPARAÃ‡ÃƒO - [Tempo estimado: 2.5 minutos]

**ğŸ‘¤ Apresentador: JoÃ£o Pedro**

### Slide 14: Metodologia de ValidaÃ§Ã£o

**ConteÃºdo do Slide:**
- **Abordagem:** ValidaÃ§Ã£o com amostra aleatÃ³ria (mesma para ambas as abordagens)

- **ConfiguraÃ§Ã£o:**
  - ğŸ“Š Amostra: 50.000 ratings
  - ğŸ‘¥ Origem: 5.000 usuÃ¡rios da amostra
  - ğŸ² Seed: 42 (reprodutibilidade)
  - ğŸ“ MÃ©todo: ComparaÃ§Ã£o real vs. predito
  - âš–ï¸ Objetivo: ComparaÃ§Ã£o justa entre User-Based e Item-Based

- **MÃ©tricas Avaliadas:**
  - MAE (Mean Absolute Error) - AcurÃ¡cia
  - RMSE (Root Mean Squared Error) - AcurÃ¡cia
  - Tempo de Treinamento - Performance
  - Tempo de PrediÃ§Ã£o - Performance

**Recursos Visuais:**
- ğŸ“Š Diagrama mostrando divisÃ£o treino/teste
- ğŸ”¬ Ãcone de microscÃ³pio representando validaÃ§Ã£o rigorosa
- âš–ï¸ BalanÃ§a representando comparaÃ§Ã£o justa

**Fala do Apresentador:**
Para validar AMBAS as abordagens de forma justa, usamos exatamente a mesma metodologia. Separamos aleatoriamente 50 mil avaliaÃ§Ãµes dos nossos 5 mil usuÃ¡rios e comparamos os ratings reais com os ratings preditos por cada modelo. Usamos seed 42 para garantir reprodutibilidade. Avaliamos quatro mÃ©tricas: MAE e RMSE para medir acurÃ¡cia, e tempo de treinamento e prediÃ§Ã£o para medir performance. Agora vamos aos resultados comparativos!

---

### Slide 15: ComparaÃ§Ã£o de AcurÃ¡cia ğŸ¯

**ConteÃºdo do Slide:**
- **RESULTADOS DE ACURÃCIA:**

| MÃ©trica | Meta | User-Based | Item-Based | Melhor |
|---------|------|------------|------------|--------|
| **MAE** | < 1.5 | **0.7682** | **0.8689** | User-Based (13% melhor) |
| **RMSE** | < 2.0 | **1.0210** | **1.1599** | User-Based (14% melhor) |
| **Status** | - | âœ… **49% melhor** | âœ… **42% melhor** | Ambas superam! |

- **InterpretaÃ§Ã£o:**
  - âœ… **User-Based:** Erro mÃ©dio de 0.77 pontos (mais preciso)
  - âœ… **Item-Based:** Erro mÃ©dio de 0.87 pontos (ainda excelente)
  - âœ… **DiferenÃ§a pequena:** Apenas 0.10 pontos de diferenÃ§a
  - âœ… **AMBAS superam as metas significativamente!**

**Recursos Visuais:**
- ğŸ“Š GrÃ¡fico de barras comparando User-Based vs Item-Based vs Meta
- âœ… Checkmarks verdes para ambas as abordagens
- ğŸ† Destaque para User-Based como mais preciso
- ğŸ“ˆ Linha mostrando que ambas superam baseline

**Fala do Apresentador:**
Aqui estÃ¡ a comparaÃ§Ã£o de acurÃ¡cia! User-Based CF alcanÃ§ou MAE de 0.7682, enquanto Item-Based ficou em 0.8689 - uma diferenÃ§a de apenas 13%. Ambas superaram LARGAMENTE as metas: User-Based foi 49% melhor que a meta, Item-Based foi 42% melhor. Na prÃ¡tica, isso significa que User-Based erra em mÃ©dia 0.77 pontos, e Item-Based erra 0.87 pontos. A diferenÃ§a Ã© pequena - apenas 0.10 pontos! EntÃ£o, em termos de acurÃ¡cia, User-Based leva uma pequena vantagem, mas AMBAS sÃ£o excelentes. Agora vamos ver a performance...

---

### Slide 16: ComparaÃ§Ã£o de Performance âš¡

**ConteÃºdo do Slide:**
- **RESULTADOS DE PERFORMANCE:**

| MÃ©trica | User-Based | Item-Based | Vantagem |
|---------|------------|------------|----------|
| **Tempo de Treinamento** | ~120s | **7.65s** | Item-Based **16x mais rÃ¡pido** |
| **Tempo de PrediÃ§Ã£o (50k)** | 107.79s | **21.63s** | Item-Based **5x mais rÃ¡pido** |
| **Tempo/PrediÃ§Ã£o** | 2.16 ms | **0.43 ms** | Item-Based **5x mais rÃ¡pido** |
| **Meta** | < 5ms | âœ… | âœ… | Ambas atendem! |

- **InterpretaÃ§Ã£o:**
  - âš¡ **Item-Based:** Muito mais rÃ¡pido em tudo
  - âœ… **User-Based:** Ainda atende requisitos de produÃ§Ã£o
  - ğŸ¯ **Trade-off:** AcurÃ¡cia vs Performance

**Recursos Visuais:**
- âš¡ GrÃ¡fico de barras mostrando diferenÃ§as de tempo (destaque para Item-Based)
- ğŸš€ Ãcone de foguete para Item-Based
- âš–ï¸ BalanÃ§a mostrando trade-off acurÃ¡cia vs performance
- âœ… Ambas atendem meta de < 5ms/prediÃ§Ã£o

**Fala do Apresentador:**
Agora a performance! E aqui Item-Based CF brilha! No treinamento, Item-Based leva apenas 7.65 segundos, enquanto User-Based leva 120 segundos - Item-Based Ã© 16 vezes mais rÃ¡pido! Nas prediÃ§Ãµes, Item-Based processa 50 mil ratings em 21 segundos, User-Based leva 107 segundos - 5 vezes mais rÃ¡pido! Por prediÃ§Ã£o individual, Item-Based leva 0.43 milissegundos, User-Based 2.16 milissegundos. Mas atenÃ§Ã£o: AMBAS atendem a meta de menos de 5 milissegundos por prediÃ§Ã£o, entÃ£o ambas sÃ£o viÃ¡veis para produÃ§Ã£o. O trade-off fica claro: User-Based Ã© 13% mais preciso, mas Item-Based Ã© 5 vezes mais rÃ¡pido. Qual escolher? Depende do cenÃ¡rio!

---

### Slide 17: AnÃ¡lise Qualitativa Comparativa

**ConteÃºdo do Slide:**

**User-Based CF:**
- âœ… **Pontos Fortes:** Maior precisÃ£o (13% melhor), maior diversidade, descoberta de conteÃºdo
- âš ï¸ **LimitaÃ§Ãµes:** 5x mais lento, escalabilidade limitada, cold start com usuÃ¡rios novos

**Item-Based CF:**
- âœ… **Pontos Fortes:** 5x mais rÃ¡pido, melhor escalabilidade, recomendaÃ§Ãµes consistentes
- âš ï¸ **LimitaÃ§Ãµes:** 13% menos preciso, menor diversidade, mais conservador

**LimitaÃ§Ãµes Comuns:**
- âš ï¸ Cold Start (novos usuÃ¡rios/animes sem histÃ³rico)
- âš ï¸ Esparsidade dos dados (densidade < 5%)
- âš ï¸ Amostra limitada a 5.000 usuÃ¡rios

**Recursos Visuais:**
- âš–ï¸ Tabela comparativa lado a lado
- ğŸ¯ Ãcones verdes para pontos fortes
- âš ï¸ Ãcones amarelos para limitaÃ§Ãµes
- ğŸ“Š GrÃ¡fico radar comparando dimensÃµes qualitativas

**Fala do Apresentador:**
Vamos fazer uma anÃ¡lise qualitativa comparativa. User-Based CF tem como pontos fortes a maior precisÃ£o - 13% melhor que Item-Based - maior diversidade nas recomendaÃ§Ãµes, e capacidade de descoberta de conteÃºdo menos conhecido. Mas Ã© 5 vezes mais lento e tem problemas de escalabilidade. Item-Based CF, por outro lado, Ã© 5 vezes mais rÃ¡pido, escala melhor porque a matriz de animes Ã© mais estÃ¡vel, e gera recomendaÃ§Ãµes mais consistentes com preferÃªncias estabelecidas. Mas Ã© 13% menos preciso e mais conservador. Ambas compartilham limitaÃ§Ãµes: cold start com novos usuÃ¡rios ou animes, esparsidade dos dados, e nossa amostra limitada a 5 mil usuÃ¡rios. A escolha entre elas depende do cenÃ¡rio de uso!

---

### Slide 18: RecomendaÃ§Ãµes de Uso ğŸ¯

**ConteÃºdo do Slide:**

**CenÃ¡rio 1: Plataforma de ProduÃ§Ã£o**
- ğŸ’¡ **RecomendaÃ§Ã£o:** Sistema HÃ­brido (70% Item-Based + 30% User-Based)
- âœ… Combina performance com diversidade

**CenÃ¡rio 2: Recursos Limitados**
- ğŸ’¡ **RecomendaÃ§Ã£o:** Item-Based CF exclusivamente
- âœ… 5x mais rÃ¡pido, apenas 13% de perda em acurÃ¡cia

**CenÃ¡rio 3: MÃ¡xima PrecisÃ£o**
- ğŸ’¡ **RecomendaÃ§Ã£o:** User-Based CF
- âœ… MAE 13% melhor, maior diversidade

**CenÃ¡rio 4: Perfis Diferentes de UsuÃ¡rios**
- ğŸ‘¶ **Novos (< 10 avaliaÃ§Ãµes):** Item-Based (mais conservador)
- ğŸ‘¤ **Moderados (10-100):** 70% Item + 30% User
- ğŸŒŸ **Ativos (100+):** 50% Item + 50% User (mÃ¡xima diversidade)

**Recursos Visuais:**
- ğŸ¯ Ãcones para cada cenÃ¡rio
- ğŸ“Š GrÃ¡fico de pizza mostrando proporÃ§Ãµes do hÃ­brido
- ğŸ‘¥ Ãcones representando diferentes perfis de usuÃ¡rios
- âœ… Checkmarks destacando recomendaÃ§Ãµes

**Fala do Apresentador:**
Com base em nossa anÃ¡lise, fazemos quatro recomendaÃ§Ãµes prÃ¡ticas. Para uma plataforma de produÃ§Ã£o real, recomendamos um sistema HÃBRIDO: 70% Item-Based para garantir performance, 30% User-Based para adicionar diversidade. Se houver recursos limitados, use Item-Based exclusivamente - vocÃª perde apenas 13% em acurÃ¡cia mas ganha 5x em velocidade. Se precisar de mÃ¡xima precisÃ£o e tiver recursos, use User-Based. E o mais interessante: adapte a abordagem ao perfil do usuÃ¡rio! UsuÃ¡rios novos com poucas avaliaÃ§Ãµes se beneficiam mais de Item-Based, que Ã© mais conservador. UsuÃ¡rios moderados funcionam bem com o hÃ­brido 70-30. E usuÃ¡rios muito ativos podem receber 50-50 para mÃ¡xima diversidade. Essa personalizaÃ§Ã£o da prÃ³pria estratÃ©gia de recomendaÃ§Ã£o Ã© um insight valioso!

---

## 6. CONCLUSÃƒO - [Tempo estimado: 1 minuto]

**ğŸ‘¤ Apresentador: Ariel**

### Slide 19: Principais Conquistas

**ConteÃºdo do Slide:**
- **Resumo das Conquistas:**

1. âœ… **Metodologia CRISP-DM** completa (6 fases)
2. âœ… **Duas abordagens implementadas e comparadas:**
   - User-Based CF: MAE 0.7682 (49% melhor que meta)
   - Item-Based CF: MAE 0.8689 (42% melhor que meta)
3. âœ… **AnÃ¡lise comparativa completa:**
   - AcurÃ¡cia: User-Based 13% melhor
   - Performance: Item-Based 5x mais rÃ¡pido
4. âœ… **Pipeline reprodutÃ­vel** (Scikit-learn)
5. âœ… **RecomendaÃ§Ãµes prÃ¡ticas** para diferentes cenÃ¡rios
6. âœ… **DocumentaÃ§Ã£o completa** e cÃ³digo aberto
7. âœ… **Aprendizados valiosos** sobre trade-offs em ML

**Recursos Visuais:**
- ğŸ† Ãcone de trofÃ©u grande e destacado
- âœ… Checkmarks verdes para cada conquista
- ğŸ“Š Recap visual dos nÃºmeros principais (ambas abordagens)
- âš–ï¸ Ãcone de balanÃ§a mostrando comparaÃ§Ã£o
- ğŸ¯ GrÃ¡fico de radar mostrando completude do projeto

**Fala do Apresentador:**
Para concluir, vamos recapitular nossas principais conquistas. Primeiro, seguimos rigorosamente a metodologia CRISP-DM, passando por todas as seis fases. Segundo, e este Ã© um diferencial importante, implementamos e comparamos DUAS abordagens de filtragem colaborativa: User-Based com MAE de 0.7682, superando a meta em 49%, e Item-Based com MAE de 0.8689, superando em 42%. Terceiro, fizemos uma anÃ¡lise comparativa completa que revelou insights valiosos: User-Based Ã© 13% mais preciso, mas Item-Based Ã© 5 vezes mais rÃ¡pido. Quarto, criamos pipelines reprodutÃ­veis usando as melhores prÃ¡ticas. Quinto, fornecemos recomendaÃ§Ãµes prÃ¡ticas para diferentes cenÃ¡rios de uso, incluindo um sistema hÃ­brido. Sexto, documentamos tudo completamente e disponibilizamos o cÃ³digo aberto no GitHub. E sÃ©timo, adquirimos aprendizados profundos sobre trade-offs em Machine Learning - nÃ£o existe soluÃ§Ã£o universalmente melhor, tudo depende do contexto!

---

### Slide 20: Agradecimentos e Contatos

**ConteÃºdo do Slide:**
- **Agradecimentos:**
  - ğŸ™ TJGO - ResidÃªncia em TI
  - ğŸ‘¨â€ğŸ« Professores e orientadores
  - ğŸ‘¥ Colegas da residÃªncia
  - ğŸŒ Comunidade Kaggle (dataset)

- **RepositÃ³rio GitHub:**
  - ğŸ“‚ github.com/sudjoao/ml-residencia-ti
  - âœ… CÃ³digo completo
  - âœ… Notebook Jupyter
  - âœ… DocumentaÃ§Ã£o
  - âœ… ApresentaÃ§Ã£o web interativa

- **Contatos:**
  - ğŸ“§ Ariel Angelo Guiliane Mendes de Almeida
  - ğŸ“§ JoÃ£o Pedro JosÃ© Santos da Silva Guedes

- **Perguntas?** ğŸ™‹â€â™‚ï¸

**Recursos Visuais:**
- ğŸ›ï¸ Logo do TJGO
- ğŸ™ Logo do GitHub
- ğŸ“Š QR Code para o repositÃ³rio
- ğŸ¨ Background profissional e agradÃ¡vel
- ğŸ‘¥ Fotos da equipe (opcional)

**Fala do Apresentador:**
Para finalizar, gostarÃ­amos de agradecer ao TJGO pela oportunidade da ResidÃªncia em TI, aos nossos professores e orientadores pelo suporte, aos colegas da residÃªncia pelas discussÃµes enriquecedoras, e Ã  comunidade Kaggle por disponibilizar o dataset. Todo o cÃ³digo do projeto estÃ¡ disponÃ­vel no GitHub no repositÃ³rio ml-residencia-ti, incluindo o notebook Jupyter completo, toda a documentaÃ§Ã£o, e atÃ© uma apresentaÃ§Ã£o web interativa que desenvolvemos. Estamos Ã  disposiÃ§Ã£o para perguntas! Muito obrigado!

---

## APÃŠNDICE: DICAS PARA A APRESENTAÃ‡ÃƒO

### GestÃ£o do Tempo (10 minutos total)

- **IntroduÃ§Ã£o:** 1 min (Slides 1-2) - **Ariel**
- **Business Understanding:** 1 min (Slides 3-4) - **Ariel**
- **Data Understanding:** 1 min (Slides 5-6) - **JoÃ£o Pedro**
- **Data Preparation:** 1 min (Slides 7-8) - **Ariel**
- **Modeling:** 2 min (Slides 9-13) - **JoÃ£o Pedro** [Inclui ambas abordagens]
- **Evaluation & Comparison:** 2.5 min (Slides 14-18) - **JoÃ£o Pedro** [ComparaÃ§Ã£o detalhada]
- **ConclusÃ£o:** 1.5 min (Slides 19-20) - **Ariel**

**Total:** 10 minutos | **Slides:** 20 (vs 17 anteriores)

### DistribuiÃ§Ã£o de Responsabilidades

**Ariel apresenta:**
- IntroduÃ§Ã£o (1 min) - Slides 1-2
- Business Understanding (1 min) - Slides 3-4
- Data Preparation (1 min) - Slides 7-8
- ConclusÃ£o (1.5 min) - Slides 19-20
- **Total: 4.5 minutos**

**JoÃ£o Pedro apresenta:**
- Data Understanding (1 min) - Slides 5-6
- Modeling - Duas Abordagens (2 min) - Slides 9-13
- Evaluation & Comparison (2.5 min) - Slides 14-18
- **Total: 5.5 minutos**

### Dicas de ApresentaÃ§Ã£o

**Antes da ApresentaÃ§Ã£o:**
- âœ… Ensaiar pelo menos 3 vezes cronometrando
- âœ… Testar todos os recursos visuais e links
- âœ… Preparar backup dos slides (PDF + online)
- âœ… Ter o notebook aberto para demonstraÃ§Ã£o (se houver tempo)

**Durante a ApresentaÃ§Ã£o:**
- ğŸ¤ Falar com clareza e entusiasmo
- ğŸ‘ï¸ Manter contato visual com a audiÃªncia
- ğŸ–±ï¸ Usar apontador laser ou cursor para destacar informaÃ§Ãµes
- â±ï¸ Monitorar o tempo discretamente
- ğŸ’¬ Pausar para respirar entre slides

**Linguagem Corporal:**
- âœ… Postura ereta e confiante
- âœ… Gestos naturais para enfatizar pontos
- âœ… Movimentar-se levemente (nÃ£o ficar estÃ¡tico)
- âœ… Sorrir e demonstrar paixÃ£o pelo projeto

**Respondendo Perguntas:**
- ğŸ‘‚ Ouvir atentamente a pergunta completa
- ğŸ¤” Pausar 2-3 segundos antes de responder
- ğŸ’¡ Ser honesto se nÃ£o souber algo
- ğŸ”„ Redirecionar para pontos fortes do projeto quando apropriado

### PossÃ­veis Perguntas e Respostas Sugeridas

**P: Por que implementaram duas abordagens ao invÃ©s de uma?**
R: Excelente pergunta! Implementar duas abordagens nos permitiu fazer uma anÃ¡lise comparativa rica e entender os trade-offs entre acurÃ¡cia e performance. Descobrimos que User-Based Ã© 13% mais preciso, mas Item-Based Ã© 5x mais rÃ¡pido. Isso nos deu insights valiosos para recomendar a soluÃ§Ã£o certa para cada cenÃ¡rio. Em projetos reais, essa comparaÃ§Ã£o Ã© fundamental!

**P: Qual das duas abordagens vocÃªs recomendam?**
R: Depende do cenÃ¡rio! Para produÃ§Ã£o com muitos usuÃ¡rios, recomendamos um sistema hÃ­brido: 70% Item-Based para garantir performance, 30% User-Based para adicionar diversidade. Se houver recursos limitados, Item-Based exclusivamente. Se precisar de mÃ¡xima precisÃ£o, User-Based. E o mais interessante: podemos adaptar a estratÃ©gia ao perfil do usuÃ¡rio - novos usuÃ¡rios recebem Item-Based (mais conservador), usuÃ¡rios ativos recebem hÃ­brido 50-50 (mÃ¡xima diversidade).

**P: Por que nÃ£o usaram Deep Learning?**
R: Optamos por Filtragem Colaborativa por trÃªs razÃµes: primeiro, Ã© mais interpretÃ¡vel e conseguimos explicar as recomendaÃ§Ãµes; segundo, nosso dataset Ã© perfeito para essa abordagem; e terceiro, querÃ­amos focar em implementaÃ§Ã£o sÃ³lida, comparaÃ§Ã£o rigorosa e validaÃ§Ã£o. Deep Learning estÃ¡ no nosso roadmap futuro!

**P: Como lidam com o Cold Start?**
R: Excelente pergunta! Atualmente, para usuÃ¡rios novos, nossa recomendaÃ§Ã£o Ã© usar Item-Based CF, que Ã© mais conservador e recomenda animes bem estabelecidos. Para usuÃ¡rios sem nenhum histÃ³rico, retornamos animes populares. Reconhecemos que Ã© uma limitaÃ§Ã£o. Nossa soluÃ§Ã£o futura Ã© implementar um sistema hÃ­brido que use metadados dos animes (gÃªneros, estÃºdio, sinopse) para fazer recomendaÃ§Ãµes iniciais.

**P: O sistema escala para milhÃµes de usuÃ¡rios?**
R: Com User-Based CF, nÃ£o - estamos limitados a 5 mil usuÃ¡rios. Mas Item-Based CF escala MUITO melhor! A matriz item-item Ã© mais estÃ¡vel (nÃºmero de animes cresce mais devagar que usuÃ¡rios), e Ã© 16x mais rÃ¡pida no treinamento. Para escalar ainda mais, precisarÃ­amos de otimizaÃ§Ãµes como processamento distribuÃ­do com Spark, cache de recomendaÃ§Ãµes com Redis, e possivelmente Matrix Factorization.

**P: Quanto tempo leva para gerar uma recomendaÃ§Ã£o?**
R: Depende da abordagem! User-Based leva 2.16 milissegundos por prediÃ§Ã£o, Item-Based leva 0.43 milissegundos - 5 vezes mais rÃ¡pido! Ambas atendem a meta de menos de 5ms. O prÃ©-processamento (calcular matriz de similaridade) Ã© feito uma vez: User-Based leva ~120 segundos, Item-Based apenas 7.65 segundos.

**P: Como validaram que as recomendaÃ§Ãµes sÃ£o realmente boas?**
R: Usamos validaÃ§Ã£o rigorosa com 50 mil avaliaÃ§Ãµes reais. Comparamos ratings preditos vs reais usando MAE e RMSE. Ambas as abordagens superaram as metas: User-Based MAE 0.77 (49% melhor), Item-Based MAE 0.87 (42% melhor). Qualitativamente, analisamos exemplos reais e verificamos se fazem sentido. TambÃ©m comparamos com baseline (sempre prever a mÃ©dia) - ambas foram 40-50% melhores!

---

## CHECKLIST FINAL PRÃ‰-APRESENTAÃ‡ÃƒO

### Slides
- [ ] Todos os 20 slides criados (vs 17 anteriores)
- [ ] Recursos visuais preparados (grÃ¡ficos, diagramas, tabelas comparativas)
- [ ] GrÃ¡ficos de comparaÃ§Ã£o User-Based vs Item-Based
- [ ] Tabelas de mÃ©tricas com ambas abordagens
- [ ] TransiÃ§Ãµes suaves configuradas
- [ ] Fontes legÃ­veis (mÃ­nimo 24pt para texto)
- [ ] Cores contrastantes e profissionais
- [ ] Logo do TJGO em todos os slides
- [ ] NumeraÃ§Ã£o de slides
- [ ] Backup em PDF e online

### ConteÃºdo
- [ ] NÃºmeros conferidos - User-Based: MAE 0.7682, RMSE 1.0210
- [ ] NÃºmeros conferidos - Item-Based: MAE 0.8689, RMSE 1.1599
- [ ] Tempos de performance verificados (5x, 16x)
- [ ] Links funcionando (GitHub, Overleaf)
- [ ] QR Codes testados
- [ ] Exemplos verificados (UsuÃ¡rio ID 48 em ambas abordagens)
- [ ] ReferÃªncias corretas

### Apresentador
- [ ] Ensaio completo realizado (3x mÃ­nimo)
- [ ] Tempo controlado (mÃ¡ximo 10 minutos)
- [ ] Fala naturalizada (nÃ£o decorada roboticamente)
- [ ] Respostas para perguntas comuns preparadas
- [ ] Roupa profissional separada
- [ ] Ãgua disponÃ­vel

### TÃ©cnico
- [ ] Notebook carregado e testado
- [ ] Projetor/tela testados
- [ ] Controle remoto/clicker funcionando
- [ ] Backup dos slides em pen drive
- [ ] Internet funcionando (para links)
- [ ] Plano B se tecnologia falhar

---

**BOA SORTE NA APRESENTAÃ‡ÃƒO! ğŸš€ğŸ¯ğŸ†**


